{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Strands Swarm Testing and Development\n",
        "\n",
        "This notebook demonstrates and tests the EdgeMind MEC orchestration system using Strands agents.\n",
        "\n",
        "## Overview\n",
        "\n",
        "- **5 Specialized Agents**: OrchestratorAgent, LoadBalancerAgent, DecisionCoordinatorAgent, ResourceMonitorAgent, CacheManagerAgent\n",
        "- **Swarm Coordination**: Real Strands multi-agent consensus for MEC site selection\n",
        "- **Threshold Integration**: ThresholdMonitor ‚Üí SwarmCoordinator ‚Üí Strands Swarm\n",
        "- **Performance Target**: Sub-100ms orchestration decisions\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "```bash\n",
        "pip install 'strands-agents[openai]' strands-agents-tools\n",
        "```\n",
        "\n",
        "Set your OpenAI API key:\n",
        "\n",
        "```bash\n",
        "export OPENAI_API_KEY=\"your-api-key-here\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import asyncio\n",
        "import time\n",
        "import json\n",
        "from datetime import UTC, datetime\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Check if OpenAI API key is set\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    print(\n",
        "        \"‚ö†Ô∏è  WARNING: OPENAI_API_KEY not set. Please set it before running Strands agents.\"\n",
        "    )\n",
        "    print(\"   export OPENAI_API_KEY='your-api-key-here'\")\n",
        "else:\n",
        "    print(\"‚úÖ OpenAI API key is configured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Strands framework\n",
        "try:\n",
        "    from strands import Agent\n",
        "    from strands.multiagent import Swarm\n",
        "\n",
        "    print(\"‚úÖ Strands framework imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import Strands: {e}\")\n",
        "    print(\n",
        "        \"   Please install: pip install 'strands-agents[openai]' strands-agents-tools\"\n",
        "    )\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import our MEC orchestration components\n",
        "try:\n",
        "    from config import ThresholdConfig\n",
        "    from src.data.metrics_generator import MECMetrics\n",
        "    from src.orchestrator.threshold_monitor import (\n",
        "        ThresholdMonitor,\n",
        "        ThresholdEvent,\n",
        "        SeverityLevel,\n",
        "        EventType,\n",
        "    )\n",
        "    from src.agents.orchestrator_agent import OrchestratorAgent\n",
        "    from src.agents.load_balancer_agent import LoadBalancerAgent\n",
        "    from src.agents.decision_coordinator_agent import DecisionCoordinatorAgent\n",
        "    from src.agents.resource_monitor_agent import ResourceMonitorAgent\n",
        "    from src.agents.cache_manager_agent import CacheManagerAgent\n",
        "    from src.swarm.swarm_coordinator import SwarmCoordinator\n",
        "\n",
        "    print(\"‚úÖ MEC orchestration components imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import MEC components: {e}\")\n",
        "    print(\"   Make sure you're running from the project root directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Test Individual Strands Agents\n",
        "\n",
        "Let's test each specialized agent individually to understand their behavior and system prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test OrchestratorAgent\n",
        "print(\"=== Testing OrchestratorAgent ===\")\n",
        "orchestrator = OrchestratorAgent(mec_site=\"MEC_A\")\n",
        "print(f\"Agent ID: {orchestrator.agent_id}\")\n",
        "print(f\"MEC Site: {orchestrator.mec_site}\")\n",
        "print(f\"Status: {orchestrator.get_agent_status()}\")\n",
        "print(f\"System Prompt Preview: {orchestrator.agent.system_prompt[:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test LoadBalancerAgent\n",
        "print(\"=== Testing LoadBalancerAgent ===\")\n",
        "load_balancer = LoadBalancerAgent(mec_site=\"MEC_B\")\n",
        "print(f\"Agent ID: {load_balancer.agent_id}\")\n",
        "print(f\"Specialization: {load_balancer.get_agent_status()['specialization']}\")\n",
        "print(f\"System Prompt Preview: {load_balancer.agent.system_prompt[:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test DecisionCoordinatorAgent\n",
        "print(\"=== Testing DecisionCoordinatorAgent ===\")\n",
        "decision_coordinator = DecisionCoordinatorAgent(mec_site=\"MEC_C\")\n",
        "print(f\"Agent ID: {decision_coordinator.agent_id}\")\n",
        "print(\n",
        "    f\"Specialization: {decision_coordinator.get_agent_status()['specialization']}\"\n",
        ")\n",
        "print(\n",
        "    f\"System Prompt Preview: {decision_coordinator.agent.system_prompt[:200]}...\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test ResourceMonitorAgent\n",
        "print(\"=== Testing ResourceMonitorAgent ===\")\n",
        "resource_monitor = ResourceMonitorAgent(mec_site=\"MEC_A\")\n",
        "print(f\"Agent ID: {resource_monitor.agent_id}\")\n",
        "print(\n",
        "    f\"Specialization: {resource_monitor.get_agent_status()['specialization']}\"\n",
        ")\n",
        "print(\n",
        "    f\"System Prompt Preview: {resource_monitor.agent.system_prompt[:200]}...\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test CacheManagerAgent\n",
        "print(\"=== Testing CacheManagerAgent ===\")\n",
        "cache_manager = CacheManagerAgent(mec_site=\"MEC_B\")\n",
        "print(f\"Agent ID: {cache_manager.agent_id}\")\n",
        "print(f\"Specialization: {cache_manager.get_agent_status()['specialization']}\")\n",
        "print(f\"System Prompt Preview: {cache_manager.agent.system_prompt[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create and Test Strands Swarm\n",
        "\n",
        "Now let's create a Strands swarm with our specialized agents and test the coordination.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Strands Swarm with our agents\n",
        "print(\"=== Creating Strands Swarm ===\")\n",
        "\n",
        "# Create the swarm with orchestrator as entry point\n",
        "swarm = Swarm(\n",
        "    agents=[\n",
        "        orchestrator.agent,\n",
        "        load_balancer.agent,\n",
        "        decision_coordinator.agent,\n",
        "        resource_monitor.agent,\n",
        "        cache_manager.agent,\n",
        "    ],\n",
        "    entry_point=orchestrator.agent,\n",
        "    max_handoffs=10,\n",
        "    max_iterations=15,\n",
        "    execution_timeout=30.0,  # 30 seconds for testing\n",
        "    node_timeout=10.0,  # 10 seconds per agent\n",
        "    repetitive_handoff_detection_window=6,\n",
        "    repetitive_handoff_min_unique_agents=3,\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Swarm created with {len(swarm.agents)} agents\")\n",
        "print(f\"Entry point: {swarm.entry_point.name}\")\n",
        "print(f\"Max handoffs: {swarm.max_handoffs}\")\n",
        "print(f\"Execution timeout: {swarm.execution_timeout}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Simple Swarm Coordination\n",
        "\n",
        "Let's test the swarm with a simple MEC orchestration scenario.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test simple swarm coordination\n",
        "print(\"=== Testing Simple Swarm Coordination ===\")\n",
        "\n",
        "simple_task = \"\"\"\n",
        "MEC ORCHESTRATION REQUEST\n",
        "\n",
        "Scenario: Gaming application experiencing high latency\n",
        "Current Site: MEC_A (CPU: 85%, GPU: 90%, Latency: 120ms)\n",
        "Available Sites: MEC_B (CPU: 60%, GPU: 55%), MEC_C (CPU: 40%, GPU: 35%)\n",
        "\n",
        "Task: Coordinate as a swarm to select the optimal MEC site for load balancing.\n",
        "Each agent should contribute their expertise:\n",
        "- LoadBalancer: Assess site capacity and performance\n",
        "- ResourceMonitor: Provide current metrics analysis\n",
        "- CacheManager: Consider model availability and cache performance\n",
        "- DecisionCoordinator: Facilitate consensus and make final decision\n",
        "\n",
        "Target: Sub-100ms total coordination time\n",
        "\"\"\"\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "try:\n",
        "    # Execute swarm coordination\n",
        "    result = swarm(simple_task)\n",
        "\n",
        "    coordination_time = (time.perf_counter() - start_time) * 1000\n",
        "\n",
        "    print(f\"\\n‚úÖ Swarm coordination completed in {coordination_time:.2f}ms\")\n",
        "    print(f\"Status: {result.status}\")\n",
        "    print(f\"Execution count: {result.execution_count}\")\n",
        "    print(f\"Execution time: {result.execution_time}ms\")\n",
        "\n",
        "    # Show agent participation\n",
        "    print(f\"\\nAgent participation:\")\n",
        "    for i, node in enumerate(result.node_history):\n",
        "        print(f\"  {i+1}. {node.node_id}\")\n",
        "\n",
        "    # Show final result\n",
        "    print(f\"\\nFinal Result:\")\n",
        "    print(\n",
        "        result.result[:500] + \"...\"\n",
        "        if len(result.result) > 500\n",
        "        else result.result\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    coordination_time = (time.perf_counter() - start_time) * 1000\n",
        "    print(f\"‚ùå Swarm coordination failed after {coordination_time:.2f}ms\")\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test Threshold-Triggered Swarm Activation\n",
        "\n",
        "Now let's test the complete integration: ThresholdMonitor ‚Üí SwarmCoordinator ‚Üí Strands Swarm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize threshold monitoring and swarm coordination\n",
        "print(\"=== Setting up Threshold-Triggered Swarm System ===\")\n",
        "\n",
        "# Create threshold configuration\n",
        "thresholds = ThresholdConfig()\n",
        "print(\n",
        "    f\"Thresholds: CPU {thresholds.cpu_threshold_percent}%, GPU {thresholds.gpu_threshold_percent}%, Latency {thresholds.latency_threshold_ms}ms\"\n",
        ")\n",
        "\n",
        "# Create threshold monitor\n",
        "monitor = ThresholdMonitor(thresholds)\n",
        "print(f\"‚úÖ ThresholdMonitor created\")\n",
        "\n",
        "# Create swarm coordinator\n",
        "coordinator = SwarmCoordinator()\n",
        "print(f\"‚úÖ SwarmCoordinator created with {len(coordinator.agents)} agents\")\n",
        "\n",
        "# Connect threshold monitor to swarm coordinator\n",
        "monitor.add_breach_callback(coordinator.activate_swarm)\n",
        "print(f\"‚úÖ Threshold monitor connected to swarm coordinator\")\n",
        "\n",
        "# Show system status\n",
        "swarm_status = coordinator.get_swarm_status()\n",
        "print(f\"\\nSystem Status:\")\n",
        "print(f\"  Swarm state: {swarm_status['state']}\")\n",
        "print(f\"  Total agents: {swarm_status['total_agents']}\")\n",
        "print(\n",
        "    f\"  Healthy MEC sites: {swarm_status['healthy_sites']}/{swarm_status['total_sites']}\"\n",
        ")\n",
        "print(f\"  Swarm available: {swarm_status['swarm_available']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test threshold breach scenario\n",
        "print(\"=== Testing Threshold Breach Scenario ===\")\n",
        "\n",
        "# Create metrics that will trigger threshold breach\n",
        "breach_metrics = MECMetrics(\n",
        "    site_id=\"MEC_A\",\n",
        "    cpu_utilization=95.0,  # Exceeds 80% threshold\n",
        "    gpu_utilization=92.0,  # Exceeds 80% threshold\n",
        "    memory_utilization=75.0,\n",
        "    queue_depth=60,  # Exceeds 50 threshold\n",
        "    response_time_ms=150.0,  # Exceeds 100ms threshold\n",
        "    network_latency={\"MEC_B\": 18.0, \"MEC_C\": 22.0},\n",
        "    timestamp=datetime.now(UTC),\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Breach metrics: CPU {breach_metrics.cpu_utilization}%, GPU {breach_metrics.gpu_utilization}%, Latency {breach_metrics.response_time_ms}ms\"\n",
        ")\n",
        "\n",
        "# Monitor thresholds (this will trigger swarm if breaches detected)\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "threshold_events = monitor.check_thresholds(breach_metrics)\n",
        "\n",
        "total_time = (time.perf_counter() - start_time) * 1000\n",
        "\n",
        "print(f\"\\nüìä Threshold Check Results:\")\n",
        "print(f\"  Events generated: {len(threshold_events)}\")\n",
        "print(f\"  Total time: {total_time:.2f}ms\")\n",
        "\n",
        "for event in threshold_events:\n",
        "    print(\n",
        "        f\"  üö® {event.metric_name}: {event.current_value} > {event.threshold_value} (severity: {event.severity.value})\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check swarm coordination results\n",
        "print(\"=== Swarm Coordination Results ===\")\n",
        "\n",
        "swarm_events = coordinator.get_event_history()\n",
        "print(f\"Swarm events generated: {len(swarm_events)}\")\n",
        "\n",
        "if swarm_events:\n",
        "    latest_event = swarm_events[-1]\n",
        "    print(f\"\\nüìã Latest Swarm Event:\")\n",
        "    print(f\"  Event type: {latest_event['event_type']}\")\n",
        "    print(f\"  Success: {latest_event['success']}\")\n",
        "    print(f\"  Duration: {latest_event['duration_ms']}ms\")\n",
        "    print(f\"  Participants: {latest_event['participants']}\")\n",
        "\n",
        "    if latest_event.get(\"decision\"):\n",
        "        decision = latest_event[\"decision\"]\n",
        "        print(f\"\\nüéØ Swarm Decision:\")\n",
        "        print(f\"  Selected site: {decision['selected_site']}\")\n",
        "        print(f\"  Confidence: {decision['confidence_score']:.2f}\")\n",
        "        print(f\"  Execution time: {decision['execution_time_ms']}ms\")\n",
        "        print(f\"  Reasoning: {decision['reasoning']}\")\n",
        "\n",
        "        if decision.get(\"participants\"):\n",
        "            print(\n",
        "                f\"  Participating agents: {', '.join(decision['participants'])}\"\n",
        "            )\n",
        "else:\n",
        "    print(\"No swarm events generated - check if thresholds were breached\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Performance Analysis\n",
        "\n",
        "Let's analyze the performance of our swarm coordination system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance analysis\n",
        "print(\"=== Performance Analysis ===\")\n",
        "\n",
        "# Get swarm metrics\n",
        "swarm_metrics = coordinator.get_swarm_metrics()\n",
        "print(f\"\\nüìà Swarm Performance Metrics:\")\n",
        "print(f\"  Total decisions: {swarm_metrics['total_decisions']}\")\n",
        "print(f\"  Total events: {swarm_metrics['total_events']}\")\n",
        "print(f\"  Agent count: {swarm_metrics['agent_count']}\")\n",
        "print(f\"  Execution timeout: {swarm_metrics['execution_timeout']}s\")\n",
        "print(f\"  Max handoffs: {swarm_metrics['max_handoffs']}\")\n",
        "\n",
        "# Analyze timing performance\n",
        "if swarm_events:\n",
        "    total_orchestration_time = sum(\n",
        "        event[\"duration_ms\"] for event in swarm_events\n",
        "    )\n",
        "    avg_orchestration_time = total_orchestration_time / len(swarm_events)\n",
        "\n",
        "    print(f\"\\n‚è±Ô∏è  Timing Analysis:\")\n",
        "    print(f\"  Total orchestration time: {total_orchestration_time:.2f}ms\")\n",
        "    print(f\"  Average orchestration time: {avg_orchestration_time:.2f}ms\")\n",
        "    print(f\"  Target: <100ms per decision\")\n",
        "\n",
        "    if avg_orchestration_time < 100:\n",
        "        print(f\"  ‚úÖ PERFORMANCE TARGET MET!\")\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è  Performance target not met - optimization needed\")\n",
        "\n",
        "# Agent status summary\n",
        "agent_status = coordinator.get_agent_status()\n",
        "print(f\"\\nü§ñ Agent Status Summary:\")\n",
        "for agent_name, status in agent_status.items():\n",
        "    print(\n",
        "        f\"  {status['agent_type']}: {status['status']} (site: {status['mec_site']})\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test Multiple Scenarios\n",
        "\n",
        "Let's test different threshold breach scenarios to see how the swarm responds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test multiple scenarios\n",
        "print(\"=== Testing Multiple Scenarios ===\")\n",
        "\n",
        "scenarios = [\n",
        "    {\n",
        "        \"name\": \"Gaming - High CPU Load\",\n",
        "        \"metrics\": MECMetrics(\n",
        "            site_id=\"MEC_A\",\n",
        "            cpu_utilization=88.0,\n",
        "            gpu_utilization=75.0,\n",
        "            memory_utilization=65.0,\n",
        "            queue_depth=45,\n",
        "            response_time_ms=85.0,\n",
        "            network_latency={\"MEC_B\": 15.0, \"MEC_C\": 20.0},\n",
        "            timestamp=datetime.now(UTC),\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Automotive - High Latency\",\n",
        "        \"metrics\": MECMetrics(\n",
        "            site_id=\"MEC_B\",\n",
        "            cpu_utilization=70.0,\n",
        "            gpu_utilization=65.0,\n",
        "            memory_utilization=60.0,\n",
        "            queue_depth=35,\n",
        "            response_time_ms=125.0,  # High latency\n",
        "            network_latency={\"MEC_A\": 25.0, \"MEC_C\": 18.0},\n",
        "            timestamp=datetime.now(UTC),\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Healthcare - Queue Overload\",\n",
        "        \"metrics\": MECMetrics(\n",
        "            site_id=\"MEC_C\",\n",
        "            cpu_utilization=75.0,\n",
        "            gpu_utilization=70.0,\n",
        "            memory_utilization=68.0,\n",
        "            queue_depth=65,  # High queue depth\n",
        "            response_time_ms=95.0,\n",
        "            network_latency={\"MEC_A\": 20.0, \"MEC_B\": 22.0},\n",
        "            timestamp=datetime.now(UTC),\n",
        "        ),\n",
        "    },\n",
        "]\n",
        "\n",
        "scenario_results = []\n",
        "\n",
        "for i, scenario in enumerate(scenarios):\n",
        "    print(f\"\\n--- Scenario {i+1}: {scenario['name']} ---\")\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    # Check thresholds\n",
        "    events = monitor.check_thresholds(scenario[\"metrics\"])\n",
        "\n",
        "    scenario_time = (time.perf_counter() - start_time) * 1000\n",
        "\n",
        "    result = {\n",
        "        \"scenario\": scenario[\"name\"],\n",
        "        \"events\": len(events),\n",
        "        \"time_ms\": scenario_time,\n",
        "        \"breaches\": [f\"{e.metric_name}:{e.current_value}\" for e in events],\n",
        "    }\n",
        "\n",
        "    scenario_results.append(result)\n",
        "\n",
        "    print(f\"  Events: {len(events)}\")\n",
        "    print(f\"  Time: {scenario_time:.2f}ms\")\n",
        "    for event in events:\n",
        "        print(\n",
        "            f\"    üö® {event.metric_name}: {event.current_value} (severity: {event.severity.value})\"\n",
        "        )\n",
        "\n",
        "# Summary\n",
        "print(f\"\\nüìä Scenario Summary:\")\n",
        "for result in scenario_results:\n",
        "    print(\n",
        "        f\"  {result['scenario']}: {result['events']} events, {result['time_ms']:.2f}ms\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualize Agent Handoffs\n",
        "\n",
        "Let's create a simple visualization of how agents hand off to each other in the swarm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize agent handoffs (if we have swarm results)\n",
        "print(\"=== Agent Handoff Visualization ===\")\n",
        "\n",
        "if swarm_events and swarm_events[-1].get(\"decision\"):\n",
        "    latest_decision = swarm_events[-1][\"decision\"]\n",
        "    participants = latest_decision.get(\"participants\", [])\n",
        "\n",
        "    print(f\"\\nüîÑ Agent Handoff Flow:\")\n",
        "    print(f\"  Entry Point: {orchestrator.agent_id}\")\n",
        "\n",
        "    if participants:\n",
        "        print(f\"  Participants: {len(participants)} agents\")\n",
        "        for i, agent_id in enumerate(participants):\n",
        "            arrow = \" ‚Üí \" if i < len(participants) - 1 else \"\"\n",
        "            print(f\"    {i+1}. {agent_id}{arrow}\")\n",
        "\n",
        "    # Create a simple text-based flow diagram\n",
        "    print(f\"\\nüìã Coordination Flow:\")\n",
        "    print(f\"  1. üö® Threshold Breach Detected\")\n",
        "    print(f\"  2. ü§ñ OrchestratorAgent activates swarm\")\n",
        "    print(f\"  3. üîÑ Agents collaborate via handoffs\")\n",
        "    print(f\"  4. üéØ DecisionCoordinator reaches consensus\")\n",
        "    print(f\"  5. ‚úÖ Decision executed: {latest_decision['selected_site']}\")\n",
        "    print(f\"  6. üìä Performance: {latest_decision['execution_time_ms']}ms\")\n",
        "else:\n",
        "    print(\"No swarm coordination results available for visualization\")\n",
        "    print(\"Try running a threshold breach scenario first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test Error Handling and Edge Cases\n",
        "\n",
        "Let's test how the system handles various error conditions and edge cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test edge cases\n",
        "print(\"=== Testing Edge Cases ===\")\n",
        "\n",
        "# Test 1: Normal metrics (no breach)\n",
        "print(\"\\n1. Testing normal metrics (no threshold breach):\")\n",
        "normal_metrics = MECMetrics(\n",
        "    site_id=\"MEC_A\",\n",
        "    cpu_utilization=45.0,\n",
        "    gpu_utilization=40.0,\n",
        "    memory_utilization=50.0,\n",
        "    queue_depth=25,\n",
        "    response_time_ms=75.0,\n",
        "    network_latency={\"MEC_B\": 15.0, \"MEC_C\": 18.0},\n",
        "    timestamp=datetime.now(UTC),\n",
        ")\n",
        "\n",
        "normal_events = monitor.check_thresholds(normal_metrics)\n",
        "print(f\"  Events generated: {len(normal_events)} (expected: 0)\")\n",
        "\n",
        "# Test 2: Extreme values\n",
        "print(\"\\n2. Testing extreme threshold breaches:\")\n",
        "extreme_metrics = MECMetrics(\n",
        "    site_id=\"MEC_A\",\n",
        "    cpu_utilization=99.0,\n",
        "    gpu_utilization=98.0,\n",
        "    memory_utilization=95.0,\n",
        "    queue_depth=100,\n",
        "    response_time_ms=500.0,\n",
        "    network_latency={\"MEC_B\": 50.0, \"MEC_C\": 60.0},\n",
        "    timestamp=datetime.now(UTC),\n",
        ")\n",
        "\n",
        "extreme_events = monitor.check_thresholds(extreme_metrics)\n",
        "print(f\"  Events generated: {len(extreme_events)}\")\n",
        "for event in extreme_events:\n",
        "    print(\n",
        "        f\"    üî• {event.metric_name}: {event.current_value} (severity: {event.severity.value})\"\n",
        "    )\n",
        "\n",
        "# Test 3: System status after multiple tests\n",
        "print(\"\\n3. Final system status:\")\n",
        "final_status = coordinator.get_swarm_status()\n",
        "final_metrics = coordinator.get_swarm_metrics()\n",
        "\n",
        "print(f\"  Swarm state: {final_status['state']}\")\n",
        "print(f\"  Total decisions made: {final_metrics['total_decisions']}\")\n",
        "print(f\"  Total events logged: {final_metrics['total_events']}\")\n",
        "print(\n",
        "    f\"  All agents operational: {final_status['total_agents'] == len(coordinator.agents)}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary and Next Steps\n",
        "\n",
        "Let's summarize our testing results and identify areas for improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"=== Testing Summary ===\")\n",
        "\n",
        "# Collect all results\n",
        "total_events = len(coordinator.get_event_history())\n",
        "total_decisions = coordinator.get_swarm_metrics()[\"total_decisions\"]\n",
        "\n",
        "print(f\"\\nüìä Test Results:\")\n",
        "print(f\"  ‚úÖ Strands agents created and configured: 5\")\n",
        "print(f\"  ‚úÖ Swarm coordination system: Operational\")\n",
        "print(f\"  ‚úÖ Threshold monitoring integration: Working\")\n",
        "print(f\"  ‚úÖ Total swarm events generated: {total_events}\")\n",
        "print(f\"  ‚úÖ Total decisions made: {total_decisions}\")\n",
        "\n",
        "# Performance assessment\n",
        "if swarm_events:\n",
        "    avg_time = sum(e[\"duration_ms\"] for e in swarm_events) / len(swarm_events)\n",
        "    performance_status = (\n",
        "        \"‚úÖ MET\" if avg_time < 100 else \"‚ö†Ô∏è  NEEDS OPTIMIZATION\"\n",
        "    )\n",
        "    print(f\"  üéØ Performance target (<100ms): {performance_status}\")\n",
        "    print(f\"     Average coordination time: {avg_time:.2f}ms\")\n",
        "\n",
        "print(f\"\\nüîß System Architecture Validation:\")\n",
        "print(f\"  ‚úÖ OrchestratorAgent: Entry point and coordination trigger\")\n",
        "print(f\"  ‚úÖ LoadBalancerAgent: MEC site selection specialist\")\n",
        "print(f\"  ‚úÖ DecisionCoordinatorAgent: Consensus management\")\n",
        "print(f\"  ‚úÖ ResourceMonitorAgent: Performance monitoring\")\n",
        "print(f\"  ‚úÖ CacheManagerAgent: Model caching optimization\")\n",
        "\n",
        "print(f\"\\nüöÄ Next Steps for Development:\")\n",
        "print(f\"  1. Implement real MCP tools (metrics_monitor, container_ops, etc.)\")\n",
        "print(f\"  2. Add comprehensive unit and integration tests\")\n",
        "print(f\"  3. Optimize swarm configuration for sub-100ms targets\")\n",
        "print(f\"  4. Create Streamlit dashboard integration\")\n",
        "print(f\"  5. Add error handling and fallback mechanisms\")\n",
        "print(f\"  6. Implement performance monitoring and alerting\")\n",
        "\n",
        "print(f\"\\n‚úÖ Strands Swarm Testing Complete!\")\n",
        "print(f\"   Ready for Task 3.5: Building comprehensive test suite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appendix: Agent System Prompts\n",
        "\n",
        "For reference, here are the complete system prompts for each agent:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display all agent system prompts for documentation\n",
        "print(\"=== Agent System Prompts ===\")\n",
        "\n",
        "agents_info = [\n",
        "    (\"OrchestratorAgent\", orchestrator),\n",
        "    (\"LoadBalancerAgent\", load_balancer),\n",
        "    (\"DecisionCoordinatorAgent\", decision_coordinator),\n",
        "    (\"ResourceMonitorAgent\", resource_monitor),\n",
        "    (\"CacheManagerAgent\", cache_manager),\n",
        "]\n",
        "\n",
        "for agent_name, agent_obj in agents_info:\n",
        "    print(f\"\\n--- {agent_name} ---\")\n",
        "    print(f\"Agent ID: {agent_obj.agent_id}\")\n",
        "    print(f\"MEC Site: {agent_obj.mec_site}\")\n",
        "    print(f\"System Prompt:\")\n",
        "    print(agent_obj.agent.system_prompt)\n",
        "    print(\"-\" * 80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
