{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Strands Swarm Testing and Development\n",
        "\n",
        "This notebook demonstrates and tests the EdgeMind MEC orchestration system using Strands agents.\n",
        "\n",
        "## Overview\n",
        "\n",
        "- **5 Specialized Agents**: OrchestratorAgent, LoadBalancerAgent, DecisionCoordinatorAgent, ResourceMonitorAgent, CacheManagerAgent\n",
        "- **Swarm Coordination**: Real Strands multi-agent consensus for MEC site selection\n",
        "- **Threshold Integration**: ThresholdMonitor → SwarmCoordinator → Strands Swarm\n",
        "- **Performance Target**: Sub-100ms orchestration decisions\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "```bash\n",
        "pip install 'strands-agents[openai]' strands-agents-tools\n",
        "```\n",
        "\n",
        "Set your OpenAI API key:\n",
        "\n",
        "```bash\n",
        "export OPENAI_API_KEY=\"your-api-key-here\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import asyncio\n",
        "import time\n",
        "import json\n",
        "from datetime import UTC, datetime\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Check if OpenAI API key is set\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    print(\n",
        "        \"⚠️  WARNING: OPENAI_API_KEY not set. Please set it before running Strands agents.\"\n",
        "    )\n",
        "    print(\"   export OPENAI_API_KEY='your-api-key-here'\")\n",
        "else:\n",
        "    print(\"✅ OpenAI API key is configured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Strands framework\n",
        "try:\n",
        "    from strands import Agent\n",
        "    from strands.multiagent import Swarm\n",
        "\n",
        "    print(\"✅ Strands framework imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ Failed to import Strands: {e}\")\n",
        "    print(\n",
        "        \"   Please install: pip install 'strands-agents[openai]' strands-agents-tools\"\n",
        "    )\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import our MEC orchestration components\n",
        "try:\n",
        "    from config import ThresholdConfig\n",
        "    from src.data.metrics_generator import MECMetrics\n",
        "    from src.orchestrator.threshold_monitor import (\n",
        "        ThresholdMonitor,\n",
        "        ThresholdEvent,\n",
        "        SeverityLevel,\n",
        "        EventType,\n",
        "    )\n",
        "    from src.agents.orchestrator_agent import OrchestratorAgent\n",
        "    from src.agents.load_balancer_agent import LoadBalancerAgent\n",
        "    from src.agents.decision_coordinator_agent import DecisionCoordinatorAgent\n",
        "    from src.agents.resource_monitor_agent import ResourceMonitorAgent\n",
        "    from src.agents.cache_manager_agent import CacheManagerAgent\n",
        "    from src.swarm.swarm_coordinator import SwarmCoordinator\n",
        "\n",
        "    print(\"✅ MEC orchestration components imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ Failed to import MEC components: {e}\")\n",
        "    print(\"   Make sure you're running from the project root directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Test Individual Strands Agents\n",
        "\n",
        "Let's test each specialized agent individually to understand their behavior and system prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test OrchestratorAgent\n",
        "print(\"=== Testing OrchestratorAgent ===\")\n",
        "orchestrator = OrchestratorAgent(mec_site=\"MEC_A\")\n",
        "print(f\"Agent ID: {orchestrator.agent_id}\")\n",
        "print(f\"MEC Site: {orchestrator.mec_site}\")\n",
        "print(f\"Status: {orchestrator.get_agent_status()}\")\n",
        "print(f\"System Prompt Preview: {orchestrator.agent.system_prompt[:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test LoadBalancerAgent\n",
        "print(\"=== Testing LoadBalancerAgent ===\")\n",
        "load_balancer = LoadBalancerAgent(mec_site=\"MEC_B\")\n",
        "print(f\"Agent ID: {load_balancer.agent_id}\")\n",
        "print(f\"Specialization: {load_balancer.get_agent_status()['specialization']}\")\n",
        "print(f\"System Prompt Preview: {load_balancer.agent.system_prompt[:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test DecisionCoordinatorAgent\n",
        "print(\"=== Testing DecisionCoordinatorAgent ===\")\n",
        "decision_coordinator = DecisionCoordinatorAgent(mec_site=\"MEC_C\")\n",
        "print(f\"Agent ID: {decision_coordinator.agent_id}\")\n",
        "print(\n",
        "    f\"Specialization: {decision_coordinator.get_agent_status()['specialization']}\"\n",
        ")\n",
        "print(\n",
        "    f\"System Prompt Preview: {decision_coordinator.agent.system_prompt[:200]}...\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test ResourceMonitorAgent\n",
        "print(\"=== Testing ResourceMonitorAgent ===\")\n",
        "resource_monitor = ResourceMonitorAgent(mec_site=\"MEC_A\")\n",
        "print(f\"Agent ID: {resource_monitor.agent_id}\")\n",
        "print(\n",
        "    f\"Specialization: {resource_monitor.get_agent_status()['specialization']}\"\n",
        ")\n",
        "print(\n",
        "    f\"System Prompt Preview: {resource_monitor.agent.system_prompt[:200]}...\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test CacheManagerAgent\n",
        "print(\"=== Testing CacheManagerAgent ===\")\n",
        "cache_manager = CacheManagerAgent(mec_site=\"MEC_B\")\n",
        "print(f\"Agent ID: {cache_manager.agent_id}\")\n",
        "print(f\"Specialization: {cache_manager.get_agent_status()['specialization']}\")\n",
        "print(f\"System Prompt Preview: {cache_manager.agent.system_prompt[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create and Test Strands Swarm\n",
        "\n",
        "Now let's create a Strands swarm with our specialized agents and test the coordination.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Strands Swarm with our agents\n",
        "print(\"=== Creating Strands Swarm ===\")\n",
        "\n",
        "# Create the swarm with orchestrator as entry point\n",
        "swarm = Swarm(\n",
        "    agents=[\n",
        "        orchestrator.agent,\n",
        "        load_balancer.agent,\n",
        "        decision_coordinator.agent,\n",
        "        resource_monitor.agent,\n",
        "        cache_manager.agent,\n",
        "    ],\n",
        "    entry_point=orchestrator.agent,\n",
        "    max_handoffs=10,\n",
        "    max_iterations=15,\n",
        "    execution_timeout=30.0,  # 30 seconds for testing\n",
        "    node_timeout=10.0,  # 10 seconds per agent\n",
        "    repetitive_handoff_detection_window=6,\n",
        "    repetitive_handoff_min_unique_agents=3,\n",
        ")\n",
        "\n",
        "print(f\"✅ Swarm created with {len(swarm.agents)} agents\")\n",
        "print(f\"Entry point: {swarm.entry_point.name}\")\n",
        "print(f\"Max handoffs: {swarm.max_handoffs}\")\n",
        "print(f\"Execution timeout: {swarm.execution_timeout}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Simple Swarm Coordination\n",
        "\n",
        "Let's test the swarm with a simple MEC orchestration scenario.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test simple swarm coordination\n",
        "print(\"=== Testing Simple Swarm Coordination ===\")\n",
        "\n",
        "simple_task = \"\"\"\n",
        "MEC ORCHESTRATION REQUEST\n",
        "\n",
        "Scenario: Gaming application experiencing high latency\n",
        "Current Site: MEC_A (CPU: 85%, GPU: 90%, Latency: 120ms)\n",
        "Available Sites: MEC_B (CPU: 60%, GPU: 55%), MEC_C (CPU: 40%, GPU: 35%)\n",
        "\n",
        "Task: Coordinate as a swarm to select the optimal MEC site for load balancing.\n",
        "Each agent should contribute their expertise:\n",
        "- LoadBalancer: Assess site capacity and performance\n",
        "- ResourceMonitor: Provide current metrics analysis\n",
        "- CacheManager: Consider model availability and cache performance\n",
        "- DecisionCoordinator: Facilitate consensus and make final decision\n",
        "\n",
        "Target: Sub-100ms total coordination time\n",
        "\"\"\"\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "try:\n",
        "    # Execute swarm coordination\n",
        "    result = swarm(simple_task)\n",
        "\n",
        "    coordination_time = (time.perf_counter() - start_time) * 1000\n",
        "\n",
        "    print(f\"\\n✅ Swarm coordination completed in {coordination_time:.2f}ms\")\n",
        "    print(f\"Status: {result.status}\")\n",
        "    print(f\"Execution count: {result.execution_count}\")\n",
        "    print(f\"Execution time: {result.execution_time}ms\")\n",
        "\n",
        "    # Show agent participation\n",
        "    print(f\"\\nAgent participation:\")\n",
        "    for i, node in enumerate(result.node_history):\n",
        "        print(f\"  {i+1}. {node.node_id}\")\n",
        "\n",
        "    # Show final result\n",
        "    print(f\"\\nFinal Result:\")\n",
        "    print(\n",
        "        result.result[:500] + \"...\"\n",
        "        if len(result.result) > 500\n",
        "        else result.result\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    coordination_time = (time.perf_counter() - start_time) * 1000\n",
        "    print(f\"❌ Swarm coordination failed after {coordination_time:.2f}ms\")\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test Threshold-Triggered Swarm Activation\n",
        "\n",
        "Now let's test the complete integration: ThresholdMonitor → SwarmCoordinator → Strands Swarm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize threshold monitoring and swarm coordination\n",
        "print(\"=== Setting up Threshold-Triggered Swarm System ===\")\n",
        "\n",
        "# Create threshold configuration\n",
        "thresholds = ThresholdConfig()\n",
        "print(\n",
        "    f\"Thresholds: CPU {thresholds.cpu_threshold_percent}%, GPU {thresholds.gpu_threshold_percent}%, Latency {thresholds.latency_threshold_ms}ms\"\n",
        ")\n",
        "\n",
        "# Create threshold monitor\n",
        "monitor = ThresholdMonitor(thresholds)\n",
        "print(f\"✅ ThresholdMonitor created\")\n",
        "\n",
        "# Create swarm coordinator\n",
        "coordinator = SwarmCoordinator()\n",
        "print(f\"✅ SwarmCoordinator created with {len(coordinator.agents)} agents\")\n",
        "\n",
        "# Connect threshold monitor to swarm coordinator\n",
        "monitor.add_breach_callback(coordinator.activate_swarm)\n",
        "print(f\"✅ Threshold monitor connected to swarm coordinator\")\n",
        "\n",
        "# Show system status\n",
        "swarm_status = coordinator.get_swarm_status()\n",
        "print(f\"\\nSystem Status:\")\n",
        "print(f\"  Swarm state: {swarm_status['state']}\")\n",
        "print(f\"  Total agents: {swarm_status['total_agents']}\")\n",
        "print(\n",
        "    f\"  Healthy MEC sites: {swarm_status['healthy_sites']}/{swarm_status['total_sites']}\"\n",
        ")\n",
        "print(f\"  Swarm available: {swarm_status['swarm_available']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test threshold breach scenario\n",
        "print(\"=== Testing Threshold Breach Scenario ===\")\n",
        "\n",
        "# Create metrics that will trigger threshold breach\n",
        "breach_metrics = MECMetrics(\n",
        "    site_id=\"MEC_A\",\n",
        "    cpu_utilization=95.0,  # Exceeds 80% threshold\n",
        "    gpu_utilization=92.0,  # Exceeds 80% threshold\n",
        "    memory_utilization=75.0,\n",
        "    queue_depth=60,  # Exceeds 50 threshold\n",
        "    response_time_ms=150.0,  # Exceeds 100ms threshold\n",
        "    network_latency={\"MEC_B\": 18.0, \"MEC_C\": 22.0},\n",
        "    timestamp=datetime.now(UTC),\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Breach metrics: CPU {breach_metrics.cpu_utilization}%, GPU {breach_metrics.gpu_utilization}%, Latency {breach_metrics.response_time_ms}ms\"\n",
        ")\n",
        "\n",
        "# Monitor thresholds (this will trigger swarm if breaches detected)\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "threshold_events = monitor.check_thresholds(breach_metrics)\n",
        "\n",
        "total_time = (time.perf_counter() - start_time) * 1000\n",
        "\n",
        "print(f\"\\n📊 Threshold Check Results:\")\n",
        "print(f\"  Events generated: {len(threshold_events)}\")\n",
        "print(f\"  Total time: {total_time:.2f}ms\")\n",
        "\n",
        "for event in threshold_events:\n",
        "    print(\n",
        "        f\"  🚨 {event.metric_name}: {event.current_value} > {event.threshold_value} (severity: {event.severity.value})\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check swarm coordination results\n",
        "print(\"=== Swarm Coordination Results ===\")\n",
        "\n",
        "swarm_events = coordinator.get_event_history()\n",
        "print(f\"Swarm events generated: {len(swarm_events)}\")\n",
        "\n",
        "if swarm_events:\n",
        "    latest_event = swarm_events[-1]\n",
        "    print(f\"\\n📋 Latest Swarm Event:\")\n",
        "    print(f\"  Event type: {latest_event['event_type']}\")\n",
        "    print(f\"  Success: {latest_event['success']}\")\n",
        "    print(f\"  Duration: {latest_event['duration_ms']}ms\")\n",
        "    print(f\"  Participants: {latest_event['participants']}\")\n",
        "\n",
        "    if latest_event.get(\"decision\"):\n",
        "        decision = latest_event[\"decision\"]\n",
        "        print(f\"\\n🎯 Swarm Decision:\")\n",
        "        print(f\"  Selected site: {decision['selected_site']}\")\n",
        "        print(f\"  Confidence: {decision['confidence_score']:.2f}\")\n",
        "        print(f\"  Execution time: {decision['execution_time_ms']}ms\")\n",
        "        print(f\"  Reasoning: {decision['reasoning']}\")\n",
        "\n",
        "        if decision.get(\"participants\"):\n",
        "            print(\n",
        "                f\"  Participating agents: {', '.join(decision['participants'])}\"\n",
        "            )\n",
        "else:\n",
        "    print(\"No swarm events generated - check if thresholds were breached\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Performance Analysis\n",
        "\n",
        "Let's analyze the performance of our swarm coordination system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance analysis\n",
        "print(\"=== Performance Analysis ===\")\n",
        "\n",
        "# Get swarm metrics\n",
        "swarm_metrics = coordinator.get_swarm_metrics()\n",
        "print(f\"\\n📈 Swarm Performance Metrics:\")\n",
        "print(f\"  Total decisions: {swarm_metrics['total_decisions']}\")\n",
        "print(f\"  Total events: {swarm_metrics['total_events']}\")\n",
        "print(f\"  Agent count: {swarm_metrics['agent_count']}\")\n",
        "print(f\"  Execution timeout: {swarm_metrics['execution_timeout']}s\")\n",
        "print(f\"  Max handoffs: {swarm_metrics['max_handoffs']}\")\n",
        "\n",
        "# Analyze timing performance\n",
        "if swarm_events:\n",
        "    total_orchestration_time = sum(\n",
        "        event[\"duration_ms\"] for event in swarm_events\n",
        "    )\n",
        "    avg_orchestration_time = total_orchestration_time / len(swarm_events)\n",
        "\n",
        "    print(f\"\\n⏱️  Timing Analysis:\")\n",
        "    print(f\"  Total orchestration time: {total_orchestration_time:.2f}ms\")\n",
        "    print(f\"  Average orchestration time: {avg_orchestration_time:.2f}ms\")\n",
        "    print(f\"  Target: <100ms per decision\")\n",
        "\n",
        "    if avg_orchestration_time < 100:\n",
        "        print(f\"  ✅ PERFORMANCE TARGET MET!\")\n",
        "    else:\n",
        "        print(f\"  ⚠️  Performance target not met - optimization needed\")\n",
        "\n",
        "# Agent status summary\n",
        "agent_status = coordinator.get_agent_status()\n",
        "print(f\"\\n🤖 Agent Status Summary:\")\n",
        "for agent_name, status in agent_status.items():\n",
        "    print(\n",
        "        f\"  {status['agent_type']}: {status['status']} (site: {status['mec_site']})\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test Multiple Scenarios\n",
        "\n",
        "Let's test different threshold breach scenarios to see how the swarm responds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test multiple scenarios\n",
        "print(\"=== Testing Multiple Scenarios ===\")\n",
        "\n",
        "scenarios = [\n",
        "    {\n",
        "        \"name\": \"Gaming - High CPU Load\",\n",
        "        \"metrics\": MECMetrics(\n",
        "            site_id=\"MEC_A\",\n",
        "            cpu_utilization=88.0,\n",
        "            gpu_utilization=75.0,\n",
        "            memory_utilization=65.0,\n",
        "            queue_depth=45,\n",
        "            response_time_ms=85.0,\n",
        "            network_latency={\"MEC_B\": 15.0, \"MEC_C\": 20.0},\n",
        "            timestamp=datetime.now(UTC),\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Automotive - High Latency\",\n",
        "        \"metrics\": MECMetrics(\n",
        "            site_id=\"MEC_B\",\n",
        "            cpu_utilization=70.0,\n",
        "            gpu_utilization=65.0,\n",
        "            memory_utilization=60.0,\n",
        "            queue_depth=35,\n",
        "            response_time_ms=125.0,  # High latency\n",
        "            network_latency={\"MEC_A\": 25.0, \"MEC_C\": 18.0},\n",
        "            timestamp=datetime.now(UTC),\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Healthcare - Queue Overload\",\n",
        "        \"metrics\": MECMetrics(\n",
        "            site_id=\"MEC_C\",\n",
        "            cpu_utilization=75.0,\n",
        "            gpu_utilization=70.0,\n",
        "            memory_utilization=68.0,\n",
        "            queue_depth=65,  # High queue depth\n",
        "            response_time_ms=95.0,\n",
        "            network_latency={\"MEC_A\": 20.0, \"MEC_B\": 22.0},\n",
        "            timestamp=datetime.now(UTC),\n",
        "        ),\n",
        "    },\n",
        "]\n",
        "\n",
        "scenario_results = []\n",
        "\n",
        "for i, scenario in enumerate(scenarios):\n",
        "    print(f\"\\n--- Scenario {i+1}: {scenario['name']} ---\")\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    # Check thresholds\n",
        "    events = monitor.check_thresholds(scenario[\"metrics\"])\n",
        "\n",
        "    scenario_time = (time.perf_counter() - start_time) * 1000\n",
        "\n",
        "    result = {\n",
        "        \"scenario\": scenario[\"name\"],\n",
        "        \"events\": len(events),\n",
        "        \"time_ms\": scenario_time,\n",
        "        \"breaches\": [f\"{e.metric_name}:{e.current_value}\" for e in events],\n",
        "    }\n",
        "\n",
        "    scenario_results.append(result)\n",
        "\n",
        "    print(f\"  Events: {len(events)}\")\n",
        "    print(f\"  Time: {scenario_time:.2f}ms\")\n",
        "    for event in events:\n",
        "        print(\n",
        "            f\"    🚨 {event.metric_name}: {event.current_value} (severity: {event.severity.value})\"\n",
        "        )\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n📊 Scenario Summary:\")\n",
        "for result in scenario_results:\n",
        "    print(\n",
        "        f\"  {result['scenario']}: {result['events']} events, {result['time_ms']:.2f}ms\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualize Agent Handoffs\n",
        "\n",
        "Let's create a simple visualization of how agents hand off to each other in the swarm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize agent handoffs (if we have swarm results)\n",
        "print(\"=== Agent Handoff Visualization ===\")\n",
        "\n",
        "if swarm_events and swarm_events[-1].get(\"decision\"):\n",
        "    latest_decision = swarm_events[-1][\"decision\"]\n",
        "    participants = latest_decision.get(\"participants\", [])\n",
        "\n",
        "    print(f\"\\n🔄 Agent Handoff Flow:\")\n",
        "    print(f\"  Entry Point: {orchestrator.agent_id}\")\n",
        "\n",
        "    if participants:\n",
        "        print(f\"  Participants: {len(participants)} agents\")\n",
        "        for i, agent_id in enumerate(participants):\n",
        "            arrow = \" → \" if i < len(participants) - 1 else \"\"\n",
        "            print(f\"    {i+1}. {agent_id}{arrow}\")\n",
        "\n",
        "    # Create a simple text-based flow diagram\n",
        "    print(f\"\\n📋 Coordination Flow:\")\n",
        "    print(f\"  1. 🚨 Threshold Breach Detected\")\n",
        "    print(f\"  2. 🤖 OrchestratorAgent activates swarm\")\n",
        "    print(f\"  3. 🔄 Agents collaborate via handoffs\")\n",
        "    print(f\"  4. 🎯 DecisionCoordinator reaches consensus\")\n",
        "    print(f\"  5. ✅ Decision executed: {latest_decision['selected_site']}\")\n",
        "    print(f\"  6. 📊 Performance: {latest_decision['execution_time_ms']}ms\")\n",
        "else:\n",
        "    print(\"No swarm coordination results available for visualization\")\n",
        "    print(\"Try running a threshold breach scenario first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test Error Handling and Edge Cases\n",
        "\n",
        "Let's test how the system handles various error conditions and edge cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test edge cases\n",
        "print(\"=== Testing Edge Cases ===\")\n",
        "\n",
        "# Test 1: Normal metrics (no breach)\n",
        "print(\"\\n1. Testing normal metrics (no threshold breach):\")\n",
        "normal_metrics = MECMetrics(\n",
        "    site_id=\"MEC_A\",\n",
        "    cpu_utilization=45.0,\n",
        "    gpu_utilization=40.0,\n",
        "    memory_utilization=50.0,\n",
        "    queue_depth=25,\n",
        "    response_time_ms=75.0,\n",
        "    network_latency={\"MEC_B\": 15.0, \"MEC_C\": 18.0},\n",
        "    timestamp=datetime.now(UTC),\n",
        ")\n",
        "\n",
        "normal_events = monitor.check_thresholds(normal_metrics)\n",
        "print(f\"  Events generated: {len(normal_events)} (expected: 0)\")\n",
        "\n",
        "# Test 2: Extreme values\n",
        "print(\"\\n2. Testing extreme threshold breaches:\")\n",
        "extreme_metrics = MECMetrics(\n",
        "    site_id=\"MEC_A\",\n",
        "    cpu_utilization=99.0,\n",
        "    gpu_utilization=98.0,\n",
        "    memory_utilization=95.0,\n",
        "    queue_depth=100,\n",
        "    response_time_ms=500.0,\n",
        "    network_latency={\"MEC_B\": 50.0, \"MEC_C\": 60.0},\n",
        "    timestamp=datetime.now(UTC),\n",
        ")\n",
        "\n",
        "extreme_events = monitor.check_thresholds(extreme_metrics)\n",
        "print(f\"  Events generated: {len(extreme_events)}\")\n",
        "for event in extreme_events:\n",
        "    print(\n",
        "        f\"    🔥 {event.metric_name}: {event.current_value} (severity: {event.severity.value})\"\n",
        "    )\n",
        "\n",
        "# Test 3: System status after multiple tests\n",
        "print(\"\\n3. Final system status:\")\n",
        "final_status = coordinator.get_swarm_status()\n",
        "final_metrics = coordinator.get_swarm_metrics()\n",
        "\n",
        "print(f\"  Swarm state: {final_status['state']}\")\n",
        "print(f\"  Total decisions made: {final_metrics['total_decisions']}\")\n",
        "print(f\"  Total events logged: {final_metrics['total_events']}\")\n",
        "print(\n",
        "    f\"  All agents operational: {final_status['total_agents'] == len(coordinator.agents)}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary and Next Steps\n",
        "\n",
        "Let's summarize our testing results and identify areas for improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"=== Testing Summary ===\")\n",
        "\n",
        "# Collect all results\n",
        "total_events = len(coordinator.get_event_history())\n",
        "total_decisions = coordinator.get_swarm_metrics()[\"total_decisions\"]\n",
        "\n",
        "print(f\"\\n📊 Test Results:\")\n",
        "print(f\"  ✅ Strands agents created and configured: 5\")\n",
        "print(f\"  ✅ Swarm coordination system: Operational\")\n",
        "print(f\"  ✅ Threshold monitoring integration: Working\")\n",
        "print(f\"  ✅ Total swarm events generated: {total_events}\")\n",
        "print(f\"  ✅ Total decisions made: {total_decisions}\")\n",
        "\n",
        "# Performance assessment\n",
        "if swarm_events:\n",
        "    avg_time = sum(e[\"duration_ms\"] for e in swarm_events) / len(swarm_events)\n",
        "    performance_status = (\n",
        "        \"✅ MET\" if avg_time < 100 else \"⚠️  NEEDS OPTIMIZATION\"\n",
        "    )\n",
        "    print(f\"  🎯 Performance target (<100ms): {performance_status}\")\n",
        "    print(f\"     Average coordination time: {avg_time:.2f}ms\")\n",
        "\n",
        "print(f\"\\n🔧 System Architecture Validation:\")\n",
        "print(f\"  ✅ OrchestratorAgent: Entry point and coordination trigger\")\n",
        "print(f\"  ✅ LoadBalancerAgent: MEC site selection specialist\")\n",
        "print(f\"  ✅ DecisionCoordinatorAgent: Consensus management\")\n",
        "print(f\"  ✅ ResourceMonitorAgent: Performance monitoring\")\n",
        "print(f\"  ✅ CacheManagerAgent: Model caching optimization\")\n",
        "\n",
        "print(f\"\\n🚀 Next Steps for Development:\")\n",
        "print(f\"  1. Implement real MCP tools (metrics_monitor, container_ops, etc.)\")\n",
        "print(f\"  2. Add comprehensive unit and integration tests\")\n",
        "print(f\"  3. Optimize swarm configuration for sub-100ms targets\")\n",
        "print(f\"  4. Create Streamlit dashboard integration\")\n",
        "print(f\"  5. Add error handling and fallback mechanisms\")\n",
        "print(f\"  6. Implement performance monitoring and alerting\")\n",
        "\n",
        "print(f\"\\n✅ Strands Swarm Testing Complete!\")\n",
        "print(f\"   Ready for Task 3.5: Building comprehensive test suite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appendix: Agent System Prompts\n",
        "\n",
        "For reference, here are the complete system prompts for each agent:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display all agent system prompts for documentation\n",
        "print(\"=== Agent System Prompts ===\")\n",
        "\n",
        "agents_info = [\n",
        "    (\"OrchestratorAgent\", orchestrator),\n",
        "    (\"LoadBalancerAgent\", load_balancer),\n",
        "    (\"DecisionCoordinatorAgent\", decision_coordinator),\n",
        "    (\"ResourceMonitorAgent\", resource_monitor),\n",
        "    (\"CacheManagerAgent\", cache_manager),\n",
        "]\n",
        "\n",
        "for agent_name, agent_obj in agents_info:\n",
        "    print(f\"\\n--- {agent_name} ---\")\n",
        "    print(f\"Agent ID: {agent_obj.agent_id}\")\n",
        "    print(f\"MEC Site: {agent_obj.mec_site}\")\n",
        "    print(f\"System Prompt:\")\n",
        "    print(agent_obj.agent.system_prompt)\n",
        "    print(\"-\" * 80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
